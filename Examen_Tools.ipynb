{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas + Python Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = pd.read_pickle('Data/happiness2020.pkl')\n",
    "csv = pd.read_csv('Data/happiness2020.pkl, decimal = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a column \n",
    "column = dataframe['column']\n",
    "\n",
    "#Mettre une colonne en minuscule\n",
    "minuscule = column.str.lower()\n",
    "\n",
    "#Insérer une colonne dans la dataframe existante \n",
    "dataframe.insert(0,'ColumnName',minuscule,True)\n",
    "\n",
    "#Drop une colonne \n",
    "dataframe = dataframe.drop('columnname', axis = 1)\n",
    "\n",
    "#Rename des colonnes\n",
    "dataframe = dataframe.rename(columns = {'Ancien_Name' : 'NewName'})\n",
    "\n",
    "#Merge des Datasets\n",
    "merging = pd.merge(dataframeA, dataframeB, how = 'outer,inner,...')\n",
    "\n",
    "#Sort Values in a columns \n",
    "sorted_columns = dataframe.sort_values('column_name', ascending = 'True, False')\n",
    "\n",
    "#Montrer qu'un certain nombre de colonnes \n",
    "sorted_columns.loc[:,['Country','happiness_score']]\n",
    "\n",
    "#groupby\n",
    "world_region = dataframe.groupby(\"world_region\")['happiness_score'].agg(['mean','count'])\n",
    "\n",
    "#Sort in ascending mode\n",
    "world_region.sort_values('mean', ascending = False)\n",
    "\n",
    "#Print in a specific mode on the row\n",
    "for idx, row in dataframe.iterrows():\n",
    "    print(\"{} - {} ({})\".format(row.world_region, row.country, row.happiness_score))\n",
    "\n",
    "#Print a result \n",
    "print(\"Percentage of countries with literacy level < 50%: {:.2%}\".format(result))\n",
    "\n",
    "#Scatter plot without Matplotlib\n",
    "dataframe.plot(x = \"happiness_score\", y=\"healthy_life_expectancy\", kind = 'scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text HANDLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NLP With spacy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index all books\n",
    "book = books[1]\n",
    "\n",
    "#put in raw text, get a Spacy object\n",
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Sentence Splitting (divise les phrases) (Je suis Fabio, Tu es chien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Tokenization (Tokenize les mots) ([‘Je’, ‘suis’, ‘Fabio’, ‘,’])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strings are encoded to hashes\n",
    "tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Part of speech tagging ([‘Je + PRON’, ‘suis + COD’, ‘Fabio’, ‘,’])\n",
    "\n",
    "CCONJ = Coordinating Conjunction\n",
    "ADP = Adposition\n",
    "DET = Determiner\n",
    "PRP = Pronoun, personal\n",
    "VBP = verb, non-3rd person singular present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = [(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 : Named entity Recognition ( reconnais London, Petersrburgh,…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 : Removing stop Words (everywhere, beforehand, and, an, or, ten, am,…] **STOP WORD REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can custom it \n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words = [token.text for token in doc if token.is_stop]  (detect it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 : Lemmatization (am -> be, streets -> street, me -> I)\n",
    "**STEMMING ++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print(token.text,'--->',token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
